**General Information**
Title: Sensor Integration and Simulation Environment for Autonomous Mobile Robot Platform "Jaska"
Author: Hai To
Instructor Name: Markku Niiranen


**Acronyms**
TECHBOOST project - research project on mobile robots helps
small and medium-sized enterprises (SMEs) with significant growth potential
to adopt, develop and systematically apply new technologies, especially
in robotics and artificial intelligence (AI).
Jaska - Metropolia's mobile robot platform
AMR - Autonomous Mobile Robot
LiDAR - Light Detection and Ranging
IMU - Inertial Measurement Unit
ROS2 - Robot Operating System 2


**Table of Contents**
Introduction 1.1 Background and Context 1.2 Thesis Objectives and Scope 1.3 Expected Outcomes and Contributions
Theoretical Framework and Technology Review 2.1. Fundamentals of Autonomous Mobile Robots (AMRs) 2.2. The Robot Operating System 2 (ROS2) Architecture 2.3. Sensor Technologies for Localization and Navigation 2.3.1. LiDAR and SLAM Algorithms (POINT-LIO) 2.3.2. Inertial Measurement Units (IMU) and Sensor Fusion 2.4. Review of Robotic Simulation Environments 2.4.1. Gazebo 2.4.2. Isaac Sim 2.4.3. Other Relevant Simulators (Webots, CoppeliaSim)
System Design and Hardware Integration 3.1. The "Jaska" Mobile Robot Platform Architecture 3.2. Motor Control System and Driver Integration (MODBUS) 3.3. Sensor Suite Integration 3.3.1. Unitree L2 LiDAR Integration 3.3.2. Stereo Camera and IMU Setup 3.4. Jaska Platform URDF Model Creation
Simulation Environment Implementation 4.1. Evaluation and Selection of a Simulation Environment 4.2. Configuration of the Chosen Simulator (e.g., Isaac Sim) 4.3. Mapping and Localization Validation in Simulation
ROS2 Software Stack Implementation 5.1. Development of ROS2 Sensor and Actuator Drivers 5.2. Implementation of Localization and Mapping 5.3. Configuration of the ROS2 Navigation Stack (NAV2) 5.4. Obstacle Avoidance and Path Planning
Experimental Results and Evaluation 6.1. Sensor Validation and System Integration Tests 6.2. Performance of Mapping and Localization in the Real World 6.3. Evaluation of Autonomous Navigation Proof-of-Concept
Discussion and Future Work 7.1. Analysis of Achieved Outcomes vs. Objectives 7.2. Challenges Encountered and Lessons Learned 7.3. Recommendations for Future Development
Conclusion References Appendices • Appendix 1: Jaska Platform Technical Specifications • Appendix 2: Source Code Repository Structure


**Introduction**
1. Introduction
1.1 Background and Context
The need for advanced autonomous systems is growing quickly, making mobile robotics an important area of study, especially for tough industrial settings like factories or mines. This Master's Thesis focuses on two key needs: combining sensors effectively and creating reliable simulations. These are essential for building and testing advanced robotic systems that can work on their own. Challenges include dealing with sensor errors, changing environments, and the need for efficient testing methods.
The main focus is a mobile robot used as a prototype to test autonomous navigation, sensor integration, and new technologies like custom wheels and improved LiDAR mapping. A big first step is getting this robot working again so it can be used for experiments. This involves connecting different sensors—such as LiDAR, cameras, and Inertial Measurement Units (IMUs)—into one system. This can be hard due to issues like mismatched data and environmental noise. Simulations are also crucial to test the robot virtually before real-world use, saving time and money.
This study builds on existing research. Quigley et al. (2009) developed the Robot Operating System (ROS), a widely used tool that helps combine sensors for autonomous robots. Thrun et al. (2005) explain how probability can improve location and mapping, showing the importance of good sensor fusion. Siciliano et al. (2009) stress the value of simulations to connect theory with practice. These studies highlight gaps in real-time sensor use and simulation accuracy, which this thesis aims to address.
To do this work, the researcher needs specific skills. A strong background in mechatronics, automation, robotics, or electrical engineering is necessary. Good knowledge of programming languages like Python and C++ is required for writing control codes and sensor programs. Familiarity with robotics tools like ROS/ROS2 is also helpful, as it makes the work easier and fits industry standards.
1.2 Thesis Objectives and Scope
The main goal of this thesis is to integrate sensors and a simulation environment for a mobile robot platform, creating a system that can operate autonomously. This means carefully adding and testing important parts like motors, cameras, LiDAR, and IMUs to ensure they work well together. The work is divided into three clear phases, moving from hardware setup to digital testing and real-world checks.
Phase 1: Hardware Foundation and Initial Perception
This phase sets up the robot’s physical parts and collects basic environment data. Tasks include adding motor controllers, a camera, LiDAR, and IMU to the robot. This involves solving technical problems, like connecting the Unitree L2 LiDAR driver and setting up a joystick with a MODBUS motor driver for manual control. For perception, the phase includes making a map using POINT-LIO SLAM, a simple mapping tool for robots with limited power. It also involves reviewing simulation tools—Gazebo, Webots, CoppeliaSim, and Isaac Sim—to pick the best one for long-term use, based on ROS2 compatibility as noted by Quigley et al. (2009).
Phase 2: Modeling and Simulation Implementation
Phase 2 focuses on building a digital copy of the robot and testing the chosen simulation tool. A key task is creating a detailed Jaska model that matches the real robot’s parts (base, LiDAR, camera, IMU). This model is used to test mapping and location in Gazebo and Isaac Sim, using probability methods from Thrun et al. (2005) to mimic real conditions. The results help choose the best simulation tool, which is then set up with the robot’s hardware to improve development.
Phase 3: Autonomous Navigation and Validation
The final phase builds and tests a complete software system for autonomy in the real world. This requires studying ROS2, an updated version of ROS designed for distributed systems, as explained by Quigley et al. (2009). The focus is on using ROS2 tools like NAV2 for navigation and avoiding obstacles. The main task is creating a ROS2 software system that handles sensor data, accurate location, and obstacle avoidance. Testing includes checking sensor fusion for navigation, leading to real-world demonstrations of autonomous movement, supported by control ideas from Siciliano et al. (2009).
1.3 Expected Outcomes and Contributions
Completing this thesis will provide important results for mobile robotics. First, it will deliver a fully working robot with integrated sensors and motor controls, ready for further tests. Second, it will set a standard simulation tool for future research, addressing the need for reliable virtual testing as suggested by Siciliano et al. (2009). Third, a basic ROS2 system will enable the robot to navigate on its own, advancing sensor-based autonomy as supported by Thrun et al. (2005). These achievements will improve the robot and add valuable knowledge to the field of industrial robotics.

**References**
Reference List (Vancouver Style)
Quigley M, Conley K, Gerkey B, Faust J, Foote T, Leibs J, et al. ROS: an open-source Robot Operating System. Proc IEEE/RSJ Int Conf Intell Robot Syst. 2009;2567-74.
Thrun S, Burgard W, Fox D. Probabilistic robotics. Cambridge (MA): MIT Press; 2005. 661 p.
Siciliano B, Sciavicco L, Villani L, Oriolo G. Robotics: modelling, planning and control. London: Springer; 2009. 632 p.

